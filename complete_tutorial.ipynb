{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1iJM8wDK5SG2FC7Shz6FxsLKu_QYi3nKw",
      "authorship_tag": "ABX9TyOQDh04uGzmVtZoAKgiYrwJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bjoxiah/finetuning-phi-3-tutorial/blob/main/complete_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üì¶  Install Required Packages"
      ],
      "metadata": {
        "id": "8KaAKmNNfqHv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üì¶ Clean and Stable Setup\n",
        "!pip uninstall -y wandb\n",
        "!pip install -q unsloth datasets pyarrow==19.0.0 # unsloth handles the other packages and dependencies"
      ],
      "metadata": {
        "collapsed": true,
        "id": "KfBXxayFPXpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üõë Disable WANDB"
      ],
      "metadata": {
        "id": "DNQdTJVUc6vX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"  # disable wandb logging"
      ],
      "metadata": {
        "id": "T-Ijm5b8Ss1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üõ† It's recommended to load unsloth first"
      ],
      "metadata": {
        "id": "jxj-fFl-dhBq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import unsloth  # Import first, as Unsloth recommends"
      ],
      "metadata": {
        "id": "ncdP9hfzQT0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ü§ô A Little House Keeping"
      ],
      "metadata": {
        "id": "Hd0rMSHflgDX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available(), torch.cuda.get_device_name(0))\n"
      ],
      "metadata": {
        "id": "9vi5--_9NKHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üóÇ Give colab access to my drive"
      ],
      "metadata": {
        "id": "XzYOsECennmD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "0CTXbJCXVcvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ Validate access to training dataset"
      ],
      "metadata": {
        "id": "5A3d7AGdn-Nv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "ds = load_dataset(\"json\", data_files=\"/content/drive/MyDrive/TrainingData/training_set_converted.jsonl\")\n",
        "print(ds[\"train\"][0])\n",
        "# Should show: {'messages': [{'role': 'system', 'content': '...'}, ...]}"
      ],
      "metadata": {
        "id": "qlgMD82tXCYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚öô Let's train!"
      ],
      "metadata": {
        "id": "qH1qt8xBoYXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from datasets import load_dataset\n",
        "\n",
        "# ---- 1Ô∏è‚É£ Load dataset ----\n",
        "dataset = load_dataset(\"json\", data_files=\"/content/drive/MyDrive/TrainingData/training_set_converted.jsonl\")\n",
        "print(f\"Dataset loaded: {len(dataset['train'])} examples\")\n",
        "print(\"Sample:\", dataset[\"train\"][0])\n",
        "\n",
        "# ---- 2Ô∏è‚É£ Load base model ----\n",
        "max_seq_length = 2048  # Phi-3 supports up to 4096, but 2048 is safer for memory\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"microsoft/phi-3-mini-4k-instruct\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=None,  # Auto-detect\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "# ---- 3Ô∏è‚É£ Prepare LoRA ----\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,  # Increased from 8 for better capacity\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                    \"gate_proj\", \"up_proj\", \"down_proj\"],  # All linear layers\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,  # Small dropout helps prevent overfitting\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",  # Memory efficient\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "# ---- 4Ô∏è‚É£ Phi-3 chat template formatting ----\n",
        "def formatting_func(examples):\n",
        "    \"\"\"\n",
        "    Uses Phi-3's official chat template format.\n",
        "    Phi-3 format: <|system|>\\n{system}<|end|>\\n<|user|>\\n{user}<|end|>\\n<|assistant|>\\n{assistant}<|end|>\\n\n",
        "    \"\"\"\n",
        "    texts = []\n",
        "    for messages in examples[\"messages\"]:\n",
        "        # Apply chat template - this handles the special tokens correctly\n",
        "        text = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=False  # We want the full conversation\n",
        "        )\n",
        "        texts.append(text)\n",
        "    return texts\n",
        "\n",
        "# Test the formatting\n",
        "print(\"\\n=== Sample Formatted Text ===\")\n",
        "sample = formatting_func({\"messages\": [dataset[\"train\"][0][\"messages\"]]})\n",
        "print(sample[0])\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# ---- 5Ô∏è‚É£ Training arguments ----\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/Output/finetuned-phi3-lora\",\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,  # Effective batch size = 8\n",
        "    warmup_steps=10,\n",
        "    num_train_epochs=3,  # 2-3 epochs typically sufficient\n",
        "    learning_rate=2e-4,\n",
        "    fp16=not torch.cuda.is_bf16_supported(),\n",
        "    bf16=torch.cuda.is_bf16_supported(),\n",
        "    logging_steps=10,\n",
        "    optim=\"adamw_8bit\",  # More memory efficient than adamw_torch\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    seed=42,\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=2,  # Only keep last 2 checkpoints\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "# ---- 6Ô∏è‚É£ Create Trainer ----\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    dataset_text_field=\"text\",  # Dummy field, we use formatting_func\n",
        "    formatting_func=formatting_func,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dataset_num_proc=2,  # Parallel processing\n",
        "    packing=False,  # Don't pack multiple examples together\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "# ---- 7Ô∏è‚É£ Train ----\n",
        "print(\"\\n=== Starting Training ===\")\n",
        "trainer.train()\n",
        "\n",
        "\n",
        "# ---- 8Ô∏è‚É£ Push to Hugging Face Hub ----\n",
        "print(\"\\n=== Pushing to Hugging Face Hub ===\")\n",
        "\n",
        "# Login to Hugging Face (run this once and enter your token)\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "login(token=userdata.get('HuggingFace')) # You'll be prompted for your HF token\n",
        "\n",
        "# Set your username and model name\n",
        "hf_username = \"bjoxiah\"  # Replace with your HF username\n",
        "model_name = \"acmetech-phi3-assistant\"  # Name for your model\n",
        "\n",
        "# Push LoRA adapter only (smallest, fastest)\n",
        "print(\"\\nüì§ Pushing LoRA adapter...\")\n",
        "model.push_to_hub(\n",
        "    f\"{hf_username}/{model_name}-lora\",\n",
        "    token=True,  # Uses your logged-in token\n",
        "    private=True,  # Set to False if you want it public\n",
        ")\n",
        "tokenizer.push_to_hub(\n",
        "    f\"{hf_username}/{model_name}-lora\",\n",
        "    token=True,\n",
        ")\n",
        "\n",
        "# Push merged 16-bit model (optional - larger but easier to use)\n",
        "print(\"\\nüì§ Pushing merged 16-bit model...\")\n",
        "model.push_to_hub_merged(\n",
        "    f\"{hf_username}/{model_name}\",\n",
        "    tokenizer,\n",
        "    save_method=\"merged_16bit\",\n",
        "    token=True,\n",
        "    private=True,\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Training Complete!\")\n",
        "print(f\"üì¶ LoRA model: https://huggingface.co/{hf_username}/{model_name}-lora\")\n",
        "print(f\"üì¶ Merged model: https://huggingface.co/{hf_username}/{model_name}\")"
      ],
      "metadata": {
        "id": "MQJEJg8fjegO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚ñ∂ Let's run a test"
      ],
      "metadata": {
        "id": "LDC4wEadrd1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# Login to Hugging Face (run this once and enter your token)\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "login(token=userdata.get('HuggingFace')) # You'll be prompted for your HF token\n",
        "\n",
        "# Load the fine-tuned model\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"bjoxiah/acmetech-phi3-assistant\",\n",
        "    max_seq_length=2048,\n",
        "    dtype=None,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "FastLanguageModel.for_inference(model)  # Enable inference mode\n",
        "\n",
        "# Test it\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are AcmeTech Corp's helpful AI assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"What is CloudManager?\"}\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
        "outputs = model.generate(inputs, max_new_tokens=128, temperature=0.7)\n",
        "print(tokenizer.decode(outputs[0]))"
      ],
      "metadata": {
        "id": "z-RNkQ54kgV-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}